AI大模型训练、HPC、高性能存储等业务应用场景提出了海量规模的计算需求，与传统数据中心业务相比，在流量模型和网络需求方面有着显著区别，驱使传统的数据中心网络向智算中心和无损网络转型。

基于RDMA的高性能网络已成为智算中心广泛应用的核心基础设施，但RDMA对于网络丢包异常敏感，如果丢包会导致网络性能急剧下降。在RoCEv2无损网络中利用PFC流控机制，实现交换机端口缓存溢出前暂停对端流量，阻止了丢包现象发生，但由于PFC需要逐级反压，效率较低，同时，PFC是一种粗粒度机制，运行在【端口+优先级】这个级别，不能细化到每一个Flow，可能会导致拥堵蔓延，进而出现不公平现象、线头阻塞、PFC死锁、PFC风暴等一系列性能问题。如果能够动态地调整每个Flow的发送速率，保持端口的队列深度比较稳定，那么就不会触发PFC Pause了，因此，就需要有基于Flow的拥塞控制算法。近年来业内聚焦在RDMA高性能网络的拥塞控制算法领域，进行了大量的前沿研究和工程实践工作。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/SN00GhdHicMLj5nwn6QNkib6VJhNNEjDez2NfDpIRs7O60mQMKKIiaDAE2ibnPmnicMA8bdEw58ywllZJfM79yFQ3ibQ/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**拥塞控制算法分类**

根据拥塞控制机制的不同，RDMA网络中的拥塞控制方案可以分为两大类：被动拥塞控制和主动拥塞控制。这里的所谓主动和被动的区分依据，主要是**主动拥塞控制以“请求和分配”方式运行；而被动拥塞控制则使用“尝试和退避/try and backoff”的方式运行**。

注：不同厂家的叫法不同，有些被动拥塞控制的改进算法，也被称为主动拥塞控制，这个我们不做深入的讨论，比如HW的NPCC（Network-based Proactive Congestion Control），NPCC支持在网络设备上智能识别拥塞状态，然后由网络设备主动向发送端服务器发送CNP报文，使发送端服务器及时降低发送报文的速率，解决了拥塞反馈路径过长的问题，而且可以准确控制发送的CNP报文个数。但按上面的分类，其本质还是被动拥塞控制，只不过对某些环节进行优化而已。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/SN00GhdHicMJ1xfOZWZ4g7GAB8W6wH3TibpFwTjF4y7wxlLsDdqoYFj8iaOm0HGpemTxjA5IGib72yf6plibYgwBLxA/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



- **被动拥塞控制**

  

被动拥塞控制又分为迭代探测和直接测量两种，迭代探测中有基于主机侧的端到端的控制方案，也有基于交换机辅助的控制方案。

迭代探测中，比较常见有基于丢包检测的CUBIC（丢包情况下才会触发的拥塞控制方案，不适用RDMA刚性兑付网络的要求，不在本文的讨论范围内）、基于ECN的DCQCN、基于时延检测的Timely、Swift等，但一个共同的特点是发送端根据网络的拥塞反馈信号，对发送速率进行调节。这类技术由于实现简单、易于部署被广泛使用，但通常被认为存在拥塞反应滞后、控制回环时间长、容易引起吞吐率震荡、速率收敛慢、误伤老鼠流等问题，因此有很大的优化空间。

直接测量的拥塞控制方案，直接测量算法的关键是利用交换机来精确测量当前的网络状态并显式反馈信息, 以便发送端快速做出拥塞反应, 并能准确地根据测量信息进行速率分配、控制网络拥塞。如基于INT遥测的HPCC，HPCC在数据面上找到了突破，通过智能网卡与交换机的配合，端到端实时抓取拥塞信息，从而精确获取实时的链路负载，并且根据精确的链路负载来计算合适的发送速率。

- **主动拥塞控制**

  

与上述网络拥塞发生后再进行拥塞控制的被动拥塞控制方案不同，主动拥塞控制方案旨在防止拥塞发生，只有网络管道具有足够的容量时才发送数据。主动拥塞控制以“请求和分配”方式运行，通过调度器主动对网络带宽进行统一的预约和分配, 以使总发送速率尽可能匹配瓶颈链路带宽，这样既可以充分利用带宽，又能防止丢包。根据调度器的是集中部署还是分布式部署，集中式调度器的方案，主要依靠集中式调度器对网络资源预约和分配，终端依据调度器的分配进行数据包发送，该方案的关键是调度器如何对数据包进行全局调度，如FastPass；分布式部署方案，又可以进一步细分为端到端的方案和逐跳的方案，在分布式端到端的拥塞控制方案中，发送端直接发送请求到接收端，由接收端预约和分配网络资源，而不需要交换机的参与；而逐跳的拥塞控制方案中，需要交换机对网络中间链路辅以检测和管理，发送端、接收端共同完成资源的分配和调度，方案的关键是如何利用交换机提供的信息来进行或辅助数据包的调度发送，分布式部署方案比较典型的如ExpressPass。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/SN00GhdHicMLj5nwn6QNkib6VJhNNEjDezYiaicrzwMJK09GfTL6Q6Jwg53jKk4fnUia6GxqHWXfPgxCecf6R8ia3mlQ/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**主要拥塞控制算法**

**基于ECN的拥塞控制**

2015年SIGCOMM会上微软发表了DCQCN，揭开了拥塞流控的研究序幕。此前，RDMA硬件仅仅依赖于传统网络的PFC反压机制来实现点到点的发送速度控制，没有网卡的配合，无法实现端到端的流控。DCQCN是在QCN和DCTCP的技术基础上，为RDMA网络设计了端到端的拥塞流控机制，DCQCN的设计前提还是基于ECN标记。DCQCN的拥塞控制过程中主要分为三部分：发送端（RP）调整流量发送速率，沿途转发交换机（CP）利用ECN标记报文携带网络链路的拥塞信息，接收端（NP）将收到拥塞标记通过CNP协议报文反馈给发送端。通过各种优化参数配置，DCQCN能实现很好的端到端拥塞控制效果，既能保证吞吐，和业务低时延。但是，DCQCN并不能消除对PFC的依赖，仍需要使用PFC做来避免丢包，只是DCQCN会大大降低PFC发生的频率，也是目前应用比较广泛的RDMA网络拥塞控制技术。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/SN00GhdHicMJ1xfOZWZ4g7GAB8W6wH3TibAym5cms3bxhbxgiap3vtjj4G6htcIyIu3LoAGph9m7BMQsicFVib0VlKQ/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**优势：**

- 分布式控制：引入了分布式控制的思想，允许数据中心网络中的交换机独立地进行拥塞检测和控制，这种分布式方法可以更好地适应大规模网络的动态性和异构性；
- 基于流实现拥塞控制，降低拥塞反馈时间，提高流启动速率和收敛速率。

**劣势：**

- 需要PFC配合使用，仍然无法避免不公平现象、线头阻塞、PFC死锁、PFC风暴等问题；
- 控制模型参数过多，性能与参数选择强相关，导致调参复杂，如DCQCN具有超过16个可调节的参数，为了更加适应不同的网络拓扑和流量环境，其参数的调整显得格外重要，不同参数下的网络吞吐会有50%以上的差异；
- 拥塞反应滞后、控制回环时间长、容易引起吞吐率震荡、速率收敛慢或者响应不准确，影响网络性能。

**基于时延或RTT的拥塞控制**

基于主机侧端到端的被动拥塞控制方案中，最具代表性的是拥塞控制算法Timely。2015年，谷歌提出了一种基于时延的拥塞控制方案Timely。Timely使用数据流的往返传递时间RTT作为量化链路拥塞的信息，并设计了一套相应的梯度调速算法。相较于传统的软件测量的RTT，谷歌方案在他们的智能网卡中集成了专有的RTT硬件测量电路，这使得RTT测量拥塞的方案得以实用化。在网络中，端到端传输延迟主要是由网络节点中的排队延迟引起的。也就是说，数据包的往返时间RTT可以体现其通过的所有队列的排队延迟，反映网络中的拥塞状态。RTT是有效的拥塞信号，相比于拥塞信号ECN，RTT不需要任何交换机进行反馈，因此也不需要对交换机进行任何修改，当网络规模较大时，也减少了对交换机进行配置、维护和调优的开销。而且不同于ECN作为单点反馈信号，RTT可以反映整条路径上的拥塞情况。Timely在发送方的网卡上即可实现，主要由三个部分构成：RTT测量引擎，速率计算引擎和速率控制引擎。当收到ACK时，智能网卡会启动RTT测量引擎以精确测量RTT值。当RTT测量引擎测得RTT后，会把RTT的值传递给速率计算引擎。速率计算引擎是拥塞控制算法的核心部分，根据RTT的梯度值计算流的发送速率。速率控制引擎再根据速率计算引擎算得的发送速率调整每条流的发送速度。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/SN00GhdHicMJ1xfOZWZ4g7GAB8W6wH3Tibr9d9xmgk6q6Zs49sQMn6Es9sp18HTLdUR1ibTugm52rHeRzwOJ5FcWA/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**优势：**

- 端到端拥塞控制，无需交换机的配合；
- 基于发送速率控制而非基于窗口更适合低延时DC网络，提高带宽利用率。

**劣势：**

- 对时钟同步的依赖：Timely对时钟同步要求较高，需要确保网络中的时钟同步性能良好，否则可能影响算法的准确性，成本高；
- 复杂性：Timely的设计相对较为复杂，需要综合考虑多个资源的调整，这可能使得实现和管理相对繁琐；
- 对RTT的变化敏感，需要合理的建模避免过反应。

**基于****INT****的拥塞控制**

2019 年，阿里云提出了一种基于带内遥测INT的拥塞控制协议HPCC。相比于DCQCN 和Timely，HPCC方法牺牲了一定的带宽引入了INT能力，同时也获得了超高精度的拥塞控制性能。HPCC可以实现快速的算法收敛以更优的利用闲置带宽，同时保持交换机始终处于近零队列，从而实现超低的数据传输延迟。

传统的拥塞控制算法主要依赖于丢包，RTT时延，以及ECN拥塞标识，发送端根据ECN等拥塞标记试探性调整发送速率，这可能导致网络收敛速度慢。当拥塞发生报文被标记指示路径拥塞程度字段时，交换机队列已缓存了一定数量的数据报文，此时再调整发送速率已经来不及了。同时，由于缺乏精准的拥塞信息，发送端试探性调整速率往往需要配合很多参数调优来保证性能，这也增加了在不同场景下的同一套流控机制调优的难度。HPCC在数据面上找到了突破，通过智能网卡与交换机的配合，端到端实时抓取拥塞信息，从而精确获取实时的链路负载，并且根据精确的链路负载来计算合适的发送速率。与DCQCN依赖定时器驱动不同，HPCC速率调整根据数据包的ACK来驱动。HPCC借助更细粒度链路负载信息并重新设计了拥塞控制算法，能够在大规模网络下快速收敛、降低对大Buffer的依赖、保证数据流的公平性。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/SN00GhdHicMJ1xfOZWZ4g7GAB8W6wH3TibWJ2Un8YEucg8J4BZiaBicUG7Az9gYKfGaicu7ml0XjjznfwKBuvqDjMCw/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**优势：**

- 拥塞控制准确度高，解决了在拥塞期间处理延迟的INT信息和对INT信息的过度反应等挑战；
- 快速收敛、带宽利用率高，可以维持超浅队列以获得超低延迟。

**劣势：**

- 网卡和交换机都需要支持INT，交换机需要提供 INT 信息，网卡需要支持处理 INT 的能力，部署成本高；
- 对增量部署不够友好。

**基于信用的拥塞控制**

2014年，J. Perry等人提出了基于集中式调度器的主动拥塞控制方案FastPass，它改变了以往通过收发端和交换机分布式解决时延问题的方式，采用集中控制的方式，从而真正实现全局最优。它在网络中设置一个集中的调度器，所有发送端都需要与调度器交互信息，从而确定传送速率和路径，以此达到没有排队延迟，高带宽利用以及网络中流之间的资源共享，这种集中控制的方式类似于通过中心的导航系统为汽车导航，能够选择最优的通行方式到达目的地。FastPass不仅要对所有网络的需求有全面的了解，还需要对每个数据包进行调度，算法开销大，不利于部署在规模大的网络中；另外，全局调度器会有单点故障的问题。

2017年I.Cho等人提出ExpressPass，它是一个端到端的基于Credit的拥塞控制协议。在发送数据包之前，ExpressPass来预先探测拥塞，从而使数据传输能够保证有界延迟和快速收敛，并且可以应对Burst的到来，与传统TCP不同的是，当需要发送时，首先需要向接收端请求Credit，当接收端回传一个Credit，发送端才会发送一个包。有点类似于先买票再上车，人票对应。ExpressPass利用交换机来限制Credit的速率从而限制发送端速率。它的核心思想是将网络传输过程中正向拥塞通过交换机漏桶算法转换成反向Credit的拥塞，同时通过对短小的Credit进行拥塞控制，进而使正向网络不丢包，从而提升网络传输性能。它的本质是通过预先探测网络中的剩余带宽，进而可以准确确定发送速度。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/SN00GhdHicMJ1xfOZWZ4g7GAB8W6wH3TibjHYvcM85dTMKticLN9gib0mhpqSozpI161LQpUwwVIstKLQ7AUzdgO6g/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**优势：**

- 保证数据传输的有界延迟和快速收敛，有效避免Burst；
- 提升浅Buffer可用性，降低丢包可能，减少重传，从而达到高性能。

**劣势：**

- 每次发送都需要等待Credit发送接收的RTT，对短流和长距网络不友好，对于短流来说，本来直接发送即可，在ExpressPass中却需要等Credit，并且有更大比例的Credit被浪费，如何更精确地控制Credit和分配是挑战。



![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/SN00GhdHicMLj5nwn6QNkib6VJhNNEjDezGqhHk1a4Q60vCwAyjpk7bzvOpEJN2DsauFibuhT70lDiaY2cEKJ0oXsA/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**总结**

业内的常见拥塞控制算法汇总如下：

- 基于ECN的CC：微软DCQCN、华为LDCP/NPCC等；
- 基于INT的CC：阿里HPCC/HPCC++、谷歌CSIG等；
- 基于RTT的CC：谷歌Timely/Swift/BBR、Copa、Nimbus、亚马逊SRD等；
- 基于Credit的CC：英伟达IB网络、FastPass、pHost、ExpressPass等。

另外，超级以太网联盟UEC也非常重视拥塞控制的方案实现，几个重点实现目标中就包括定义了一个可选的基于接收端的拥塞控制，它给发送端分配信用Credit，从而增强了基于发送端的拥塞控制，另外，也定了端到端遥测，端网协同的拥塞控制方案，可选支持对交换机的高级遥测进行增强，可缩短控制平面的信令时间，从而能够快速感知短时拥塞事件并做出反应，这种快速反应时间对于较高的链路速度尤其重要。

综上所述，各种类型的拥塞控制方案，在公平性、收敛速度、带宽利用、稳定性、鲁棒性、队列长度、兼容性、易用性、成本等方面各有优劣，很难一概而论，目前的拥塞控制协议很难在兼容性、易用性基础上，还具备完善的功能、性能，这也是各种拥塞控制算法层出不穷的关键原因。在实际网络环境中，不同应用场景的环境参数差异非常大，即使在同一应用下，网络环境参数也可能发生显著的变化，各种拥塞控制算法在不同场景下的表现也会各有优劣，尤其在极端条件下体现尤为显著，因此，可以基于不同的网络环境选择不同的拥塞控制算法，有时可以多种算法结合使用，可以预见，随着RDMA拥塞控制算法方面的进一步研究，将会出现比现有机制性能更好或更加适用于特定场景的RDMA拥塞控制算法。

------

随着AI技术的发展，各种新型智能应用爆发，以AI为特征的智能算力发展速度远超通用算力，据IDC预测，未来5年，我国智能算力规模的年复合增长率将达50%以上，数据中心的算力时代已经到来。层出不求的需求也对数据中心网络或智算网络提出了更高的要求，需要更大的规模、更大的带宽、更低的时延和更可靠的网络。

数据中心网络或智算网络的拓扑结构相对规整（如Spine-Leaf架构），所以在选路方面相对简单。但由于数据中心网络的服务器之间往往都存在多条等价并行路径（比如在Fat tree网络中可能存在多达几十条），因此，如何在多条并行路径之间进行负载均衡路由，是数据中心网络路由设计的关键。

传统负载均衡用的比较多的是等价成本多路径（ECMP），ECMP下一跳的路径是通过对数据包字段进行哈希计算并对可用路径数取模来选择的，来自一个流的数据包总是映射到同一路径，因此，这些数据包将按发送顺序交付，也就是通常的基于流的负载均衡（Flow-based）。然而，研究表明，在流量高度倾斜时，ECMP无法平均分配负载到不同的路径，如大象流场景。特别是对于当前HPC和AI场景，普遍使用RDMA并且是大带宽场景，这个时候传统ECMP很容易导致链路Hash不均，进而导致某一个链路拥塞。在AI/ML的应用中，GPU或其他类型的AI/ML计算单元之间他们有着非常简单的通讯关系（流的数量非常少），并且由于他们有着极高的计算能力，导致一对通讯单元间的数据吞吐极高（单个流很大，所需的网络带宽极大），这就导致在这样的应用中存在极端的负载分担不均衡，而且这种不均衡一旦引发网络丢包，就会对整体AI/ML的任务完成时间带来显著的负面影响。

网络中的拥塞整体一般有两种，一种是在端侧的拥塞，常见于多打一的Incast场景，这种情况通常用各种拥塞控制算法来使对应的发送端减速来解决。另一种是矩阵拥塞，即网络因为Hash不均导致的拥塞。这里重点讨论矩阵拥塞的解决方案。解决矩阵拥塞主要有以下几种方式：

- 胖树架构设计：增加汇聚链路带宽，典型的就是采用Fat tree网络架构设计，从输入到输出1:1的收敛比设计；
- VoQ（Virtual Output Queueing）技术：是一种网络拥塞管理技术，用于防止HoL阻塞的技术，在传统的输入缓冲区排队方案中，数据包先进入输入缓冲区，然后根据目的端口的可用性从中选择出队。然而，当多个数据包的目的端口相同时，输入缓冲区排队会导致阻塞和拥塞。VoQ技术通过为每个输出端口创建虚拟的输出队列，将输入数据包直接放入对应的虚拟输出队列中。这样，在数据包进入路由器或交换机时就可以直接选择适当的虚拟输出队列，而无需等待目的端口的可用性。因此，VoQ技术可以避免输入缓冲区排队可能引起的阻塞和拥塞问题，提高网络的吞吐量和性能；
- 负载均衡（Load balance），也是本文介绍的重点，不同的负载均衡路由策略会对端到端的吞吐率造成很大的影响。流量Load balance按照粒度不同，可以分为以下几种方式：基于流（Flow based）、基于包（Packet based）、基于流片（Flowlet）、基于信元（Cell based）



![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/SN00GhdHicMIhrcg1Lrl3pfCPq5WFKsxGoxZ0uY7xxINaZ29zvRPwHyVm7zzlB0ibvRWLQM3oZg2nxlQsLRZRyOg/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



**基于流（Flow based）**

基于流的负载均衡路由是以流为单位，把不同的流路由到不同的等价路径上。传统的方法是等价多路径（ECMP）路由，主要通过哈希的方法进行路径选择。但由于该方法将大流和小流等同对待，造成不同路径的带宽利用率相差较大，并且哈希算法的冲突问题难以实现精确的负载均衡。在实际应用中存在如下问题：

- 在流量大小分布均匀的情况下，ECMP效果较好。然而，在同时存在大象流和老鼠流的情况下，ECMP的效果并不理想。这就导致在AI/ML类应用中存在极端的负载分担不均衡，而且这种不均衡一旦引发网络丢包，就会对整体AI/ML的任务完成时间带来显著的负面影响；
- 可能加重网络链路的拥塞问题。由于ECMP仅使用哈希或轮询方法进行负载均衡，它无法感知到链路的拥塞情况。因此，在已经存在拥塞的链路上使用ECMP可能会进一步加剧拥塞情况；
- ECMP无法解决非对称网络的性能损失。当数据中心网络发生故障时，网络结构可能会出现非对称情况导致无法实现网络物理链路的均衡分布，进而造成流量不平衡的问题。



因此，尽管ECMP是一种简单的负载均衡策略，优点是没有乱序问题，但它存在上述问题，限制了其在某些场景下的有效性。在解决这些问题时，可以考虑使用更复杂的负载均衡策略或结合其他技术来改善网络性能和流量分配的均衡性。

由于ECMP工程复杂度低、性能可接受，仍然广泛应用在数据中心网络中。业界也提出了许多对ECMP进行改进的方法，比如通过集中控制器对大流进行优化的路径选择等（Hedera方案、BurstBalancer方案）。

**基于包（Packet based）**

随机包喷洒（Random Packet Spraying，RPS）是一种基于包级别的负载均衡策略。当交换机发现有多条等价路径指向同一目的地址时，RPS会将数据包以单个包为单位分散到这些路径上。与ECMP不同，RPS以数据包为单位进行操作，将同一流中的不同数据包转发到不同的等价路径上。

RPS的优点在于简单易实施，通过细粒度的负载均衡，可以在多条并行路径之间实现较为均衡的路由选择，提升端到端的网络吞吐率，可以将并行链路利用率提高到90%以上。缺点在于可能会造成同一个流的包乱序问题，所以这种方式必须要很好解决乱序问题。例如英伟达的解决方案中，就使用BlueField-3 DPU通过DDP（直接数据放置）处理无序数据，从而形成端到端的完整解决方案。

**基于流片（Flowlet）**

Flowlet本质是利用TCP的流突发特性，根据设置一定间隔将流分割为一个个Burst子流，然后每次切换都是在这个间隔中间，从而避免乱序。但是这个方式也有局限性：首先，Flowlet无法应对短连接场景，试想如果一个Flow一共就一个Burst，那Flowlet必然无法产生效果；其次，Flowlet是针对TCP的特性设计的，而RDMA流量并不符合相关特征，因此，在上层应用为RoCE流量时，Flowlet基本没有效果，这也是在AI/ML等场景中不使用Flowlet的原因。

Flowlet的实现原理如下：对于TCP Flow，我们通常假设Packet是平滑发送的，然而实际上，不管是实际抓包还是从具体实现上看，你都会发现TCP Packet的发送其实是Burst的发送的，如下图：

![图片](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

Flowlet实现原理上，可以把一条Flow看成是多个Flowlet组成的，负载均衡就是基于Flowlet来进行了，引入了一个中间层，它既不是Packet，也不是Flow，而是大于Packet小于Flow的Flowlet。那么到底如何定量去切分Flowlet呢？已知两条链路的延迟分别为D1，D2，设一个数α，当α满足下面的条件：α >|D1−D2|，一条Flow便可以通过α来切割为不同的Flowlet。

![图片](http://mmbiz.qpic.cn/sz_mmbiz_jpg/SN00GhdHicMIhrcg1Lrl3pfCPq5WFKsxG2rWKibQP4BfoLwdoHeh3muYHk733tXJD512VbJ9MmP0YhRueStG6fRQ/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1&retryload=3)

**基于信元（Cell based）**

在基于信元交换的网络级负载均衡机制下，接收端设备接收到报文后，会将报文拆分成的若干信元，信元会基于目的端发送的调度信令选择空闲的链路进行转发，到的目的后，信元被重新拼装成报文发出设备。在这样的机制下，不同于流转发，一个固定的流仅能利用单条路径，信元交换是动态的基于微观负载实时调整的均衡利用。

信元交换的粒度比基于包的负载均衡还要细，理论上，带宽利用率可以更高。信元交换本身并不是一项崭新的技术，在目前广泛应用的框式设备中，线卡芯片与网板芯片之间的流量交换普遍都采用了信元交换的技术，以实现机框内无阻塞交换。不过信元交换以前主要应用在框式设备系统内部，往往都是各个交换机设备厂商自定义的信元格式和调度机制，不具备跨厂商互通的能力，此项技术可以进一步扩展，应用到整个网络上。2019年AT&T向OCP提交了基于商用芯片的盒式路由器规范，提出了DDC（Disaggregated Distributed Chassis）的概念，DDC使用的核心技术也是信元交换的方案。

AI/ML网络支撑的业务其特征是流数量少，单条流的带宽大，同时流量不均匀，经常出现多打一或者多打多的情况（All-to-All和All-Reduce）。所以极易出现流量负载不均、链路利用率低、频繁的流量拥塞导致的丢包等问题，无法充分释放算力。信元交换将报文切片成Cells（对比Packet based的负载均衡方案，粒度更小），并根据可达信息采用轮询机制发送，流量负载会较为均衡的分配到每一条链路，实现带宽的充分利用，这样可以解决中大小流的问题，仍然存在相当多的缺陷：

- 静态时延增加；DDC的大缓存能力将报文缓存，势必增加硬件转发静态时延。同时信元交换，对报文的切片、封装和重组，同样增加网络转发时延。通过测试数据比较，DDC较传统ETH网转发时延增大1.4倍。显然不适应AI计算网络的需求。
- 硬件依赖特定芯片实现，封闭、专用；
- 大缓存设计增加网络成本；



基于上述分析可以看出以上几种负载均衡方案，各有优劣，从实际部署的角度看，负载均衡的粒度从小到大的顺序是Cell based、Packet based、Flowlet、Flow based，对应的带宽利用率则是从高到低，但是由于Cell based自身的限制，实际在智算领域基本没有应用。

当前高性能计算、分布式存储、人工智能等应用均采用RoCEv2协议来降低CPU的处理和时延，提升应用的性能。然而，由于RDMA的提出之初是承载在无损的InfiniBand网络中，RoCEv2协议缺乏完善的丢包保护机制，对于网络丢包异常敏感。尽管可以使用PFC、ECN等流量控制和拥塞控制技术尽量实现无损网络特性，但还是很难应对大象流/老鼠流分布不均的影响，而网络负载均衡是一个绕不开的点。

在这方面，各厂家也在做各种努力尝试。

比如在英伟达的RoCE解决方案中，可以针对RDMA流和TCP流采用不同的策略，如TCP使用Flowlet，RDMA（RoCE）采用逐包的负载均衡策略。Spectrum可以通过网络侧交换机和端侧DPU的紧耦合联动，做到实时动态监控ECMP各个链路的物理带宽和端口出口拥塞情况，来做到基于每个报文的动态负载分担。Spectrum-4交换机负责选择每个数据包基于最低拥塞端口，均匀分配数据传输。当同一流的不同数据包通过网络的不同路径传输时，它们可能以无序的方式到达目的地。BlueField-3 DPU通过DDP处理无序数据，避免了数据报文缓存和重组的困扰，通过上述方案，在超大规模系统和高负载场景下相对传统RoCE方案性能提升明显。再比如，华为的智能无损网络，通过ACC（Automatic ECN）动态调整每个交换机的标记阈值，以分布式方式工作，并结合离线和在线训练以适应动态流量模式，ACC在线速率下实现了老鼠流和大象流的低流完成时间，优先调度小流的报文，从而保障小流的转发时延，提升整体吞吐性能。

![image-20240530225852737](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20240530225852737.png)
