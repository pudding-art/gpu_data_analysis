可能CXL type2 memory expander card就代表CXL type3的memory expander中的内容。

![image-20240819105222128](/Users/hong/Library/Application%20Support/typora-user-images/image-20240819105222128.png)

DRAM读延迟计算的时候使用了以上两种PMU events，以上两种是什么意思？Read Pending Queue Allocation读队列的分配情况和实际的队列占用情况。

![image-20240819105639008](/Users/hong/Library/Application%20Support/typora-user-images/image-20240819105639008.png) l3 miss不包括在本地CXL type2 memory或者其他非DRAM的内存中。

运行一个使用CXL内存的程序，然后使用perf监控所有和CXL event name中包含CXL的事件。

```shell 
perf stat -e, --event <event> # event selector, use 'perf list' to list available event 多个用分号划分
```

Kernel PMU event

retired instructions https://blog.csdn.net/dongyanxia1000/article/details/74910205 *计数执行过程中消耗的指令数*![image-20240819111320826](/Users/hong/Library/Application%20Support/typora-user-images/image-20240819111320826.png)

cache下的和memory相关的指令。

![image-20240819111434586](/Users/hong/Library/Application%20Support/typora-user-images/image-20240819111434586.png)

查看一下这里的本地和远程是否和NUMA相关？

![image-20240819111545631](/Users/hong/Library/Application%20Support/typora-user-images/image-20240819111545631.png)

![image-20240819111716043](/Users/hong/Library/Application%20Support/typora-user-images/image-20240819111716043.png)

和命中CXL加速器的内存区域相关的事件

<img src="/Users/hong/Library/Application%20Support/typora-user-images/image-20240819111952798.png" alt="image-20240819111952798" style="zoom:200%;" />

![image-20240819112038652](/Users/hong/Library/Application%20Support/typora-user-images/image-20240819112038652.png)

有很多这种card writing/reading to DRAM这些，这里的card指的是啥？/：/

![image-20240819112413896](/Users/hong/Library/Application%20Support/typora-user-images/image-20240819112413896.png)

![image-20240819162453232](/Users/hong/Library/Application%20Support/typora-user-images/image-20240819162453232.png)





### 带宽调控工具与内核集成

Intel RDT相关的开源工具retest

https://blog.csdn.net/gitblog_00033/article/details/139020383

```c
#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/cgroup.h>
#include <linux/slab.h>


static int __init cgroup_cpu_control_init(void)
{
    printk(KERN_INFO "Initializing cgroup CPU control module\n");
    return 0;
}

static void __exit cgroup_cpu_control_exit(void)
{
    printk(KERN_INFO "Exiting cgroup CPU control module\n");
}


static int cgroup_cpu_control_set_quota(struct cgroup *cgrp, unsigned long quota)
{
    struct cgroup_subsys_state *css = cgroup_css(cgrp, cpu_subsys_id);

    if (!css)
        return -EINVAL;

    css->cgroup->cpu.cfs_quota_us = quota;
    return 0;
}

static int cgroup_cpu_control_set_period(struct cgroup *cgrp, unsigned long period)
{
    struct cgroup_subsys_state *css = cgroup_css(cgrp, cpu_subsys_id);

    if (!css)
        return -EINVAL;

    css->cgroup->cpu.cfs_period_us = period;
    return 0;
}

static int __init cgroup_cpu_control_init(void)
{
    printk(KERN_INFO "Initializing cgroup CPU control module\n");

    struct cgroup_subsys cpu_subsys = {
        .name = "cpu",
        .set_quota = cgroup_cpu_control_set_quota,
        .set_period = cgroup_cpu_control_set_period,
    };

    return cgroup_register_subsys(&cpu_subsys);
}

static void __exit cgroup_cpu_control_exit(void)
{
    printk(KERN_INFO "Exiting cgroup CPU control module\n");
    cgroup_unregister_subsys(&cpu_subsys);
}

module_init(cgroup_cpu_control_init);
module_exit(cgroup_cpu_control_exit);

MODULE_LICENSE("GPL");
MODULE_AUTHOR("Your Name");
MODULE_DESCRIPTION("A simple cgroup CPU control module");

```





```c
#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/kthread.h>
#include <linux/delay.h>
#include <linux/time.h>

// 假设cxl_write_request是一个函数，用于发送写请求到CXL设备
// 它接受一个指向要写入的内存的指针和要写入的数据大小作为参数
int cxl_write_request(void *mem_ptr, size_t size);

// 用于存储写请求的时间戳
struct timeval start_time, end_time;

// 线程工作函数
int write_request_thread(void *data) {
    void *mem_ptr = kmalloc(1024, GFP_KERNEL); // 分配内存
    if (!mem_ptr) {
        return -ENOMEM;
    }

    while (!kthread_should_stop()) {
        // 记录写请求开始时间
        do_gettimeofday(&start_time);

        // 发送写请求
        cxl_write_request(mem_ptr, 1024);

        // 等待写操作完成
        wait_for_completion(&write_completion);

        // 记录写请求结束时间
        do_gettimeofday(&end_time);

        // 计算延迟时间
        long mtime, seconds, useconds;
        seconds = end_time.tv_sec - start_time.tv_sec;
        useconds = end_time.tv_usec - start_time.tv_usec;
        mtime = ((seconds)*1000 + useconds/1000.0) + 0.5;

        printk(KERN_INFO "Write request completed with latency: %ld ms\n", mtime);

        // 定期发送写请求，这里使用msleep来简化示例
        msleep(100); // 100毫秒发送一次
    }

    kfree(mem_ptr);
    return 0;
}

static struct task_struct *thread;

static int __init cxl_write_test_init(void) {
    thread = kthread_run(write_request_thread, NULL, "write_request_thread");
    if (IS_ERR(thread)) {
        return PTR_ERR(thread);
    }
    return 0;
}

static void __exit cxl_write_test_exit(void) {
    kthread_stop(thread);
}

module_init(cxl_write_test_init);
module_exit(cxl_write_test_exit);

MODULE_LICENSE("GPL");
MODULE_AUTHOR("Your Name");
MODULE_DESCRIPTION("CXL Write Request Latency Test");




#include <linux/sched.h>
#include <linux/wait.h>

void wait_for_completion(struct completion *comp) {
    // 1. 检查是否已经完成
    if (comp->done)
        return;

    // 2. 设置当前任务为等待状态
    current->state = TASK_UNINTERRUPTIBLE;

    // 3. 添加当前任务到等待队列
    add_wait_queue(&comp->wait, current);

    // 4. 休眠等待，直到被唤醒
    schedule();

    // 5. 从等待队列中移除当前任务
    remove_wait_queue(&comp->wait, current);

    // 6. 检查并确保完成标志已设置
    if (!comp->done)
        BUG();
}
```

如果从内核态判断是否写入成功，应该研究一下kernel CXL driver的写法。

大多数内核函数在执行写入函数之后会返回一个完成信号来确认写入是否成功。如果使用信号量，可以在写入操作完成之后等待返回时间来确认是否完成写操作。

```c
//如果kmalloc失败，是用numa的lib分配内存
#include <linux/kernel.h>
#include <linux/numa.h>
#include <linux/completion.h>

// 分配NUMA内存
void *mem = numa_alloc_onnode(MEMSIZE_IN_BYTES, node_id);
if (!mem) {
    // 错误处理
}

// 初始化completion结构
struct completion write_completion;

// 发送写请求到CXL内存的函数
void send_cxl_write_request(void *mem, size_t size) {
    // 发送写请求的逻辑
    // ...
}

// 内核线程或定时器中定期发送写请求
while (1) {
    // 发送写请求
    send_cxl_write_request(mem, MEMSIZE_IN_BYTES);

    // 等待写请求完成
    wait_for_completion(&write_completion);

    // 计算完成时间
    unsigned long long start_time, end_time;
    start_time = /* 获取当前时间 */;
    send_cxl_write_request(mem, MEMSIZE_IN_BYTES);
    end_time = /* 获取当前时间 */;
    printk(KERN_INFO "Write request latency: %llu ns\n", (end_time - start_time));

    // 定期发送，例如每100ms
    msleep(100);
}

// 释放内存
numa_free(mem, MEMSIZE_IN_BYTES);
```

计算当前读写比例下对应的系统整体延迟？

1. **监控应用的读写活动**：使用Linux内核的监控工具，比如`/proc`文件系统，来跟踪应用的读写请求。`/proc/pid/io`文件可以提供关于进程读写操作的统计信息 47。
2. **确定读写比例**：通过分析`/proc/pid/io`中的读字节数（rchar）和写字节数（wchar），可以计算出应用的读写比例。
3. **测量延迟**：可以使用内核提供的高精度时间戳功能，如`ktime_get()`，来测量特定操作的延迟。例如，可以记录下读取或写入操作开始和结束的时间戳，然后计算两者的差值来得到延迟 47。
4. **分析延迟数据**：收集一定量的延迟样本后，可以计算平均延迟、最大延迟、以及延迟的分布情况，从而得到整体延迟的评估。
5. **考虑延迟与读写比例的关系**：如果应用的读写操作具有不同的延迟特性，需要根据读写比例对总延迟进行加权计算。例如，如果读操作占60%，写操作占40%，并且读延迟是R，写延迟是W，则整体延迟可以估算为 `0.6R + 0.4W`。
6. **编写内核代码**：可以通过编写内核模块来实现上述功能。内核模块可以访问`/proc`文件系统，使用高精度时间戳，并且可以直接与应用的读写路径进行交互。

```c
#include <linux/kernel.h>
#include <linux/proc_fs.h>
#include <linux/seq_file.h>
#include <linux/time.h>

// 定义一个结构体来存储读写延迟的统计数据
struct latency_stats {
    unsigned long long read_latency_sum;
    unsigned long long write_latency_sum;
    unsigned int read_count;
    unsigned int write_count;
};

// 内核模块初始化和退出函数
static int __init measure_latency_init(void) {
    // 初始化操作
    return 0;
}

static void __exit measure_latency_exit(void) {
    // 清理操作
}

// 测量特定操作延迟的函数
void measure_operation_latency(struct latency_stats *stats, int is_read) {
    ktime_t start, end;
  // 记录操作开始时间
  start = ktime_get();
  // 执行读或写操作...
  // 记录操作结束时间
  end = ktime_get();
  // 计算延迟并更新统计数据
  if (is_read) {
      stats->read_latency_sum += ktime_to_ns(end) - ktime_to_ns(start);
      stats->read_count++;
  } else {
      stats->write_latency_sum += ktime_to_ns(end) - ktime_to_ns(start);
      stats->write_count++;
  }
}

module_init(measure_latency_init);
module_exit(measure_latency_exit);
```



```python
# interference_detection
def interference_detection():
  for type in memory_access_type:
    lat[type] > lat_threshold[type]:
      return true
  return false 

# which host?
def host_detection():
  for type in memory_access_type:
    bw[type] < bw_threshold[type]:
      return false
  return true 

# which app in the selected host?
# read -> write or overall ops?
def selected_app():
	write_bw = []
  read_bw = []
  for cg in cgroups:
    if cg.priority == HIGH_PRIORITY:
      continue
    else:
      write_bw.insert(cg.write_bw)
      read_bw.insert(cg.read_bw)
	for i in write_bw:
    if i != 0:
  		return cgroups[get_max(write_bw)]
    else continue
  return cgroups[get_max(read_bw)]

# mba和cgroup的调控接口是什么？调控步长是多少？
def throttling_mba():
  for clos in MBA_CLOS:
    res = bind_clos_to_cgroup(clos, cgroup);
    if res:
      if interference_detection():
        continue
      else break
    else:
      return false
  return true


def throttling_cgroup():
  if mba_throttling_value == 10:
    return false
  else:
    # 直接在当前判断出contention的cgroup中限制这两个值
    for val_quota, val_period in quota, period: 
      cpu.cfg_quota_us = val_quota
      cpu.cfg_period_us =  val_period
    	# 调用cpulimit接口
     	if interference_detection():
        continue
      else break
  return true 


def relax_limitation():
	if interference_detection():
    return false
  else:
    relax_cgroup()
    relax_mba()
 		return true
```



In the prevention strategy, 将所有超过默认分配的带宽limit的cgroup全部划分为noisy neighbors;

In the remedy strategy, 先判断是否有拥塞发生，然后根据写-读的类型不同，先看写操作，再看读操作，判断当前发生的最拥塞的应用；

判断具体是哪个应用发生拥塞？

这种先判断写再判断读操作的方式是否有问题？所有的写操作和所有的读操作数先比较，然后再比较写操作数里多的，还是说只要有写操作就比较写操作，然后再比较读操作？不一定对啊。

还有就是在MBA和cgroup进行带宽调控的时候，stride是多少？

如何在kernel代码中应用Intel MBA对应用的带宽进行调控？



-----

参考文献：

https://www.kernel.org/doc/Documentation/x86/resctrl.rst

https://www.cnblogs.com/wodemia/p/17745666.html

User Iinterface for Resource Control feature

首先RDT通过CONFIG_X86_CPU_RESCTRL和x86b /proc/cpuinfo flag bits可以看出是否支持：

```shell
cat /proc/cpuinfo | grep mba
```

![image-20240818152938947](/Users/hong/Library/Application%20Support/typora-user-images/image-20240818152938947.png)

![image-20240818153017022](/Users/hong/Library/Application%20Support/typora-user-images/image-20240818153017022.png)

当前CPU确实支持MBA功能。

![image-20240818153208764](/Users/hong/Library/Application%20Support/typora-user-images/image-20240818153208764.png)

resctrl文件夹下即存在COS1-COS7的控制组。以下为当前服务器上的所有CPU核心编号，和已使能的CPU核心数。

![image-20240818153251438](/Users/hong/Library/Application%20Support/typora-user-images/image-20240818153251438.png)

schemata中有和MBA的throttling value设置相关的位置；

![image-20240818153405580](/Users/hong/Library/Application%20Support/typora-user-images/image-20240818153405580.png)

/sys/fs/resctrl/tasks下为当前配置绑定的进程PIDs。size代表的可能是使用的大小？

![image-20240818153501160](/Users/hong/Library/Application%20Support/typora-user-images/image-20240818153501160.png)

mode也不知道是什么意思？

![image-20240818153527306](/Users/hong/Library/Application%20Support/typora-user-images/image-20240818153527306.png)

![image-20240818154626282](/Users/hong/Library/Application%20Support/typora-user-images/image-20240818154626282.png)

每个COSn文件夹下的内容都和/sys/fs/resctrl下的一致.

问题：COS0是什么？resctrl本身吗？

Resource groups are represented as directories in the resctrl file system. The default group is the root directory which, immediately after mounting, owns all the tasks and cpus in the system and can make full use of all resources. COS0应该就是default group, 如果其他的不生效，默认使用COS0.

问题：如果自己创建COS8，会在pqos的输出中显示出对应的信息吗？

不会。COS实际上是RMID的一种中间表征，CPU对RMID的支持是有限的，每个CPU对RMID的支持不同，即使创建了COS8，底层的硬件无法支持COS8，仍然无法对想要监控的资源进行监控。

```shell
mount -t resctrl resctrl [-o cdp[,cdpl2][,mba_MBps]] /sys/fs/resctrl

# "mba_MBps": Enable the MBA Software Controller(mba_sc) to specify MBA bandwidth in MBps
```

![image-20240818154329973](/Users/hong/Library/Application%20Support/typora-user-images/image-20240818154329973.png)

最小带宽：用户可以发出的最小带宽。

带宽粒度



AMD的服务器支持慢内存带宽分配，主要是对CXL内存进行操作。

![image-20240818155146848](/Users/hong/Library/Application%20Support/typora-user-images/image-20240818155146848.png)







 一定要注意的是不能直接访问用户空间数据，内核代码可以通过特殊的函数来访问用户空间数据，copy_to_user copy_from_user这两个函数就是内核代码访问用户空间数据的函数，但是内核不能直接通过像是memcpy函数来直接操作用户空间数据。

  按照linux设备驱动书本上的说法，我的理解，三个原因描述如下。

  其一，驱动程序架构不同或者内核的配置不同，用户空间数据指针可能运行在内核模式下根本就是无效的，可能没有那个虚拟地址映射到的物理地址，也有可能直接指向一些随机数。

  其二，用户空间的内存数据是分页的，运行在内核模式下的用户空间指针可能直接就不在内存上，而是在swap交换的其他存储设备上，这样就会发生页面错误。页面错误是内核所不允许的，会导致该进程死亡。

  其三，内核代码访问用户内存指针，就给内核开了后门，用户程序可以利用这一点来任意的访问操作全部地址空间，这样内核就没有安全性可言了。


```c
// tgroup.h
#include <linux/cgroup.h>
#include <linux/uaccess.h>

#define MAX_PRIORITY 2

struct tgroup {
    struct cgroup_subsys_state css;
    int priority;
    unsigned long long last_bandwidth;
    unsigned long long limit[4]; // 4种内存访问类型的带宽限制
};

// 内联函数将cgroup_subsys_state转换为tgroup
static inline struct tgroup *css_to_tgroup(struct cgroup_subsys_state *css)
{
    return container_of(css, struct tgroup, css);
}

// 文件操作结构体
struct cgroup_file {
    struct cgroup_file *next;
    char *name;
    umode_t mode;
    struct cgroup_subsys_state *css;
};

// tgroup.c
#include "tgroup.h"
// 其他包含的头文件...

static struct cgroup_subsys tgroup_subsys;

// 定义cgroup文件操作
static struct cgroup_file files[] = {
    {
        .name = "cgroup.procs",
        .mode = 0644,
    },
    {
        .name = "priority",
        .mode = 0644,
    },
    {
        .name = "bandwidth",
        .mode = 0444,
    },
    {
        .name = "limit",
        .mode = 0644,
    },
    { NULL },
};

// 实现cgroup子系统的生命周期回调函数
static struct cgroup_subsys_state *tgroup_create(struct cgroup *cgroup)
{
    struct tgroup *tgrp = kzalloc(sizeof(*tgrp), GFP_KERNEL);
    if (!tgrp)
        return ERR_PTR(-ENOMEM);

    tgrp->priority = 0; // 默认优先级
    tgrp->last_bandwidth = 0;
    memset(tgrp->limit, 0, sizeof(tgrp->limit));

    return &tgrp->css;
}

static void tgroup_destroy(struct cgroup_subsys_state *css)
{
    struct tgroup *tgrp = css_to_tgroup(css);
    kfree(tgrp);
}

// 实现cgroup文件的打开和释放函数
static int tgroup_file_open(struct inode *inode, struct file *file)
{
    struct cgroup_file *cfile = PDE_DATA(inode);
    cgroup_file_get(cfile);
    file->private_data = cfile;
    return 0;
}

static int tgroup_file_release(struct inode *inode, struct file *file)
{
    cgroup_file_put(file->private_data);
    return 0;
}

// 实现写优先级的函数
static ssize_t tgroup_priority_write(struct file *file, const char __user *buffer,
                                    size_t count, loff_t *ppos)
{
    // 从用户空间读取数据并设置优先级
    // ...
    return count;
}

// 实现读取带宽的函数
static ssize_t tgroup_bandwidth_read(struct file *file, char __user *buffer,
                                     size_t count, loff_t *ppos)
{
    // 读取并返回带宽数据
    // ...
    return count;
}

// 实现写带宽限制的函数
static ssize_t tgroup_limit_write(struct file *file, const char __user *buffer,
                                  size_t count, loff_t *ppos)
{
    // 解析输入的带宽限制并设置
    // ...
    return count;
}

// 注册文件操作
static int tgroup_add_files(struct cgroup_subsys *ss, struct cgroup *cgroup,
                            struct cgroup_subsys_state *css)
{
    struct tgroup *tgrp = css_to_tgroup(css);
    int i;

    for (i = 0; files[i].name; i++) {
        struct cgroup_file *cfile = &files[i];
        cfile->css = css;
        if (cgroup_add_file(cgroup, ss, cfile->name, cfile->mode,
                            &tgroup_file_open, &tgroup_file_release,
                            cfile->write ? &tgroup_file_write : NULL,
                            cfile->read ? &tgroup_file_read, cfile) < 0)
            goto fail;
    }
    return 0;

fail:
    while (--i >= 0) {
        cgroup_remove_file(cgroup, ss, files[i].name);
    }
    return -ENOMEM;
}

static void tgroup_remove_files(struct cgroup_subsys *ss, struct cgroup *cgroup,
                               struct cgroup_subsys_state *css)
{
    int i;
    for (i = 0; files[i].name; i++)
        cgroup_remove_file(cgroup, ss, files[i].name);
}

// 实现cgroup子系统操作
static struct cgroup_subsys_ops tgroup_subsys_ops = {
    .create = tgroup_create,
    .destroy = tgroup_destroy,
    .add_files = tgroup_add_files,
    .remove_files = tgroup_remove_files,
};

static int __init tgroup_init(void)
{
    tgroup_subsys.ops = &tgroup_subsys_ops;
    tgroup_subsys.css_alloc = sizeof(struct tgroup);
    tgroup_subsys.css_free = NULL;
    tgroup_subsys.subsys_id = cgroup_subsys_id++;

    return cgroup_subsys_register(&tgroup_subsys);
}

static void __exit tgroup_exit(void)
{
    cgroup_subsys_unregister(&tgroup_subsys);
}

module_init(tgroup_init);
module_exit(tgroup_exit);
```



绑定CLOS：

```c
// 定义CLOS和MBA throttling value的映射
int clos_to_mba_throttling_value[] = {
    // 为每个CLOS分配一个MBA throttling value
    // 跳过70和80，因为它们的效果与其他值相似
    10, 20, 30, 40, 50, 60, 70, 100 // 假设100是一个有效的throttling value
};


#define NUM_CLOS ARRAY_SIZE(clos_to_mba_throttling_value)

// 在tgroup结构体中添加CLOS信息
struct tgroup {
    // ... 其他字段 ...
    int clos; // 当前分配给TGroup的CLOS
};

// 函数：将TGroup的所有线程分配到对应的CLOS
static void assign_clos_to_tgroup(struct tgroup *tgrp) {
    struct task_struct *task, *tmp;

    // 遍历TGroup中的所有任务
    list_for_each_entry_safe(task, tmp, &tgrp->css.cgroup.tasks, cgroup_tasks) {
        // 设置任务的CLOS
        // 这通常涉及到写硬件寄存器或使用特定的CPU指令
        // 以下代码是伪代码，具体实现取决于硬件
        set_task_clos(task, tgrp->clos);
    }
}

// 函数：更改TGroup的MBA throttling value
static void change_mba_throttling(struct tgroup *tgrp, int mba_value) {
    // 找到对应的CLOS
    int clos = 0;
    for (; clos < NUM_CLOS; ++clos) {
        if (clos_to_mba_throttling_value[clos] == mba_value) {
            break;
        }
    }

    if (clos >= NUM_CLOS) {
        // 无效的MBA throttling value
        return;
    }

    // 更新TGroup的CLOS
    tgrp->clos = clos;
    // 分配CLOS给TGroup的所有线程
    assign_clos_to_tgroup(tgrp);
}

// 调度器的上下文切换逻辑的一部分
static void context_switch(struct task_struct *prev, struct task_struct *next) {
    // ... 上下文切换的代码 ...

    // 在上下文切换时更改CLOS
    struct tgroup *tgrp = css_to_tgroup(cgroup_get(&next->cgroup));
    change_mba_throttling(tgrp, tgrp->clos);
}
```





```c
#include <linux/module.h>
#include <linux/kmod.h>
#include <linux/fs.h>
#include <linux/cgroup.h>
#include <linux/resctrl.h>
#include <linux/uaccess.h>

#define MBA_SETTING "MB:0=20" // 示例MBA设置

struct cgroup_subsys_state *resctrl_cgroup_alloc(struct cgroup *cgroup);
void resctrl_cgroup_free(struct cgroup_subsys_state *css);

static struct cgroup_subsys resctrl_subsys = {
    .name = "resctrl",
    .create = resctrl_cgroup_alloc,
    .destroy = resctrl_cgroup_free,
};

// 假设的resctrl_cgroup结构体，用于存储每个cgroup的MBA设置
struct resctrl_cgroup {
    struct cgroup_subsys_state css;
    // 其他字段，例如MBA设置等
};

static int __init resctrl_init(void)
{
    int ret;

    ret = cgroup_subsys_register(&resctrl_subsys);
    if (ret)
        return ret;

    // 其他初始化代码，例如创建/sys/fs/resctrl

    return 0;
}

static void __exit resctrl_exit(void)
{
    // 清理代码，例如卸载resctrl文件系统
    cgroup_subsys_unregister(&resctrl_subsys);
}

module_init(resctrl_init);
module_exit(resctrl_exit);

struct cgroup_subsys_state *resctrl_cgroup_alloc(struct cgroup *cgroup)
{
    struct resctrl_cgroup *rcg;

    rcg = kzalloc(sizeof(*rcg), GFP_KERNEL);
    if (!rcg)
        return ERR_PTR(-ENOMEM);

    // 初始化MBA设置等

    return &rcg->css;
}

void resctrl_cgroup_free(struct cgroup_subsys_state *css)
{
    struct resctrl_cgroup *rcg = css_to_resctrl_cgroup(css);

    kfree(rcg);
}

// 示例文件操作函数
static ssize_t resctrl_store_setting(struct kernfs_open_file *of, const char *buffer, size_t count)
{
    struct resctrl_cgroup *rcg = of->file->private_data;
    // 解析buffer中的MBA设置字符串
    // 应用MBA设置到硬件

    return count;
}

static struct kernfs_ops resctrl_ops = {
    .write = resctrl_store_setting,
};

static int __init resctrl_add_files(void)
{
    struct kernfs_node *node;

    // 创建/sys/fs/resctrl/schemata文件
    node = kernfs_create_file("schemata", 0644, resctrl_root, NULL, &resctrl_ops, NULL);
    if (!node)
        return -ENOMEM;

    return 0;
}

static void resctrl_remove_files(void)
{
    // 移除/sys/fs/resctrl/schemata文件
    kernfs_remove("schemata", resctrl_root);
}

// 模块加载时添加文件
module_add_action(resctrl_init, resctrl_add_files);
// 模块卸载前移除文件
module_exit_action(resctrl_exit, resctrl_remove_files);


```



```c
#include <linux/module.h>
#include <linux/kmod.h>
#include <linux/fs.h>
#include <linux/cgroup.h>
#include <linux/resctrl.h>
#include <linux/uaccess.h>

#define MBA_SETTING "MB:0=20" // 示例MBA设置

struct cgroup_subsys_state *resctrl_cgroup_alloc(struct cgroup *cgroup);
void resctrl_cgroup_free(struct cgroup_subsys_state *css);

static struct cgroup_subsys resctrl_subsys = {
    .name = "resctrl",
    .create = resctrl_cgroup_alloc,
    .destroy = resctrl_cgroup_free,
};

// 假设的resctrl_cgroup结构体，用于存储每个cgroup的MBA设置
struct resctrl_cgroup {
    struct cgroup_subsys_state css;
    // 其他字段，例如MBA设置等
};

static int __init resctrl_init(void)
{
    int ret;

    ret = cgroup_subsys_register(&resctrl_subsys);
    if (ret)
        return ret;

    // 其他初始化代码，例如创建/sys/fs/resctrl

    return 0;
}

static void __exit resctrl_exit(void)
{
    // 清理代码，例如卸载resctrl文件系统
    cgroup_subsys_unregister(&resctrl_subsys);
}

module_init(resctrl_init);
module_exit(resctrl_exit);

struct cgroup_subsys_state *resctrl_cgroup_alloc(struct cgroup *cgroup)
{
    struct resctrl_cgroup *rcg;

    rcg = kzalloc(sizeof(*rcg), GFP_KERNEL);
    if (!rcg)
        return ERR_PTR(-ENOMEM);

    // 初始化MBA设置等

    return &rcg->css;
}

void resctrl_cgroup_free(struct cgroup_subsys_state *css)
{
    struct resctrl_cgroup *rcg = css_to_resctrl_cgroup(css);

    kfree(rcg);
}

// 示例文件操作函数
static ssize_t resctrl_store_setting(struct kernfs_open_file *of, const char *buffer, size_t count)
{
    struct resctrl_cgroup *rcg = of->file->private_data;
    // 解析buffer中的MBA设置字符串
    // 应用MBA设置到硬件

    return count;
}

static struct kernfs_ops resctrl_ops = {
    .write = resctrl_store_setting,
};

static int __init resctrl_add_files(void)
{
    struct kernfs_node *node;

    // 创建/sys/fs/resctrl/schemata文件
    node = kernfs_create_file("schemata", 0644, resctrl_root, NULL, &resctrl_ops, NULL);
    if (!node)
        return -ENOMEM;

    return 0;
}

static void resctrl_remove_files(void)
{
    // 移除/sys/fs/resctrl/schemata文件
    kernfs_remove("schemata", resctrl_root);
}

// 模块加载时添加文件
module_add_action(resctrl_init, resctrl_add_files);
// 模块卸载前移除文件
module_exit_action(resctrl_exit, resctrl_remove_files);
```



https://github.com/shenango/caladan-all

https://github.com/yaochengx/EMBA







## cgroup代码解读

Linux进程调度-组调度以及带宽控制  https://www.cnblogs.com/LoyenWang/p/12459000.html

task_group代表进程组，task_struct代表进程；task_group有2种调度器，分别是CFS调度器和rt组调度。

task_struct和task_group都有权重的概念，调度器会根据权重来分配CPU的时间；进程组的权重设置，可以通过/sys文件系统进行设置，比如操作/sys/fs/cgroup/cpu/A/shares

内核中使用struct cfs_bandwidth来描述带宽，该结构包含在struct task_group中；此外，struct cfs_rq中也有与带宽控制相关的字段。

带宽控制的原理是通过task_group中的cfs_bandwidth来管理一个全局的时间池，分配给属于这个任务组的运行队列（因为队列中的每个元素的处理时间是一致的，所以时间池的分配就约等于任务的队列占用情况），当超过限额的时候则限制队列的调度。同时，cfs_bandwidth维护两个定时器，一个用于周期性的填充限额并进行时间分发处理，一个用于将未用完的时间再返回到时间池中。



所以新建的subsystem的group的bandwidth只读文件后面要加PEBS的处理函数将最终的带宽值写在bandwidth文件中(read-only)；





cgroup源码精讲 https://blog.csdn.net/hu1610552336/article/details/118642410

所以实际上cgroup的各种对内存资源的控制都是将task_group进程组中和资源相关的字段提供了一个kernel接口和管理工具给上层用户使用。

进程绑定如何实现？每个进程默认都绑定了每个cgroup子系统的顶层目录cgroup，将进程绑定到新的cgroup目录时，实际上只是从CPU cgroup顶层目录转移到新的cgroup目录而已。

如何维护大量不同线程与不同cgroup有绑定关系？引入新的数据结构struct css_set。



![image-20240819060443483](/Users/hong/Library/Application%20Support/typora-user-images/image-20240819060443483.png)

**如果shell命令对应的执行函数顺序是cgroup_attach_task->xxx, 如果在内核态直接绑定，是否直接调用cgroup_attach_task?**





## 内核RDT框架

找对应命令的内核态实现代码？？？

内核resctrl文件系统(RDT框架）以及对RDT底层硬件的抽象及实现 https://blog.csdn.net/qq_38350702/article/details/137795676

内核RDT框架如何使用？

主要是看resctrl的源码







### 一个简单的kernel module

https://www.acwing.com/blog/content/21727/

Linux操作系统的内核是单一体系结构monolithic kernel的，整个内核是一个独立的非常大的程序，与之对应的是微内核体系结构micro kernel，比如Windows NT采用的就是微内核体系结构。微内核对应操作系统的核心部分是一个很小的内核，实现一些基本服务，如创建和删除进程、内存管理、中断管理等













```shell
// 初始化参数
period = 100000  // 100ms
quota = 计算基于目标带宽百分比的配额值
burst = 0  // 根据需要调整

// 设置cgroup参数
设置_cgroup_param("cpu.cfs_period_us", period)
设置_cgroup_param("cpu.cfs_quota_us", quota)
设置_cgroup_param("cpu.cfs_burst_us", burst)

// 主监控循环
while 真：
    // 读取统计数据
    stats = 获取_cgroup_stats("cpu.stat")
    
    // 分析是否需要调整
    if stats.nr_throttled / stats.nr_periods > 阈值：
        // 增加配额
        quota = quota * 调整系数
        设置_cgroup_param("cpu.cfs_quota_us", quota)
    
    elif stats.nr_throttled / stats.nr_periods < 较低阈值：
        // 减少配额
        quota = quota / 调整系数
        设置_cgroup_param("cpu.cfs_quota_us", quota)
    
    // 休眠一段时间然后再次检查
    休眠(监控间隔)

// 辅助函数：设置cgroup参数
设置_cgroup_param(param, value)：
    写入值到文件(param, value)

// 辅助函数：获取cgroup统计数据
获取_cgroup_stats(stat_file)：
    读取并解析文件内容(stat_file)
    返回解析后的统计数据
```



![image-20240819145117145](/Users/hong/Library/Application%20Support/typora-user-images/image-20240819145117145.png)

可以先随便写一个公式带有调整系数的，最后调控效果比较好就可以，说是经验模型即可。



















































































































