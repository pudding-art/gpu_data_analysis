## 步骤一：实验环境搭建和重要信息采集

1. Experimental setup部分，首先需要一个原型系统的框图，类似Fig 8
   - Experimental setup部分原型系统的框图，需要将FPGA和ASIC CXL memory都插在同一台机器上在拍照，还是ASIC CXL memory的一张图，FPGA的另一张图？
   - 内存大小问题，为保证FPGA，ASIC CXL memory和模拟器的内存大小一致，内存设成多大？现在的FPGA上的内存大小是16GB；ASIC  CXL memory 1片有4条32G的内存条，识别为2个node，每个node64G。目前想设成32G内存，FPGA上的16GB内存条换成32GB内存条，ASIC CXL memory每个node只保留1条32G的内存条, 服务器上的本地内存也只留一个通道的内存条；
   - 查看当前系统各项参数，如下Linux kernel版本，CPU type，CPU cores数量，本地内存类型、大小和通道数、L1 dcache, L1 icache, L2 cache, LLC的大小；
   
     <img src="/Users/hong/Library/Application%20Support/typora-user-images/image-20240702122733933.png" alt="image-20240702122733933" style="zoom:50%;" />	



> [!CAUTION]
>
> 注意，在进行以下操作之前关闭hyper-threading, 需root权限：
>
> ```shell
> # 操作系统下关闭CPU超线程
> echo off > /sys/devices/system/cpu/smt/control
> ```

## 步骤二：LMbench测试延迟

`Lmbench` 是一款简易可以移植的内存测试工具，其主要功能有，带宽测评（读取缓存文件、拷贝内存、读/写内存、管道、TCP），延时测评（上下文切换、网络、文件系统的建立和删除、进程创建、信号处理、上层系统调用、内存读入反应时间）等功能。

执行单个测试用例：进入`/bin`目录下执行对应用例即可

- 这里主要关心延迟测试`lat_mem_rd`
- `lat_mem_rd`
  - 参数说明
    - `-P`：指定并行度，默认为1
    - `-t`：指定为随机访问，否则为顺序访问
    - `-N`：指定重复测试次数，默认为10次
    - `size`：访问的最大数据块，单位MB
    - `stride`：访问步长，单位Byte，默认64B
  - 示例：`./lat_mem_rd -P 2 -t -N 3 128 256`，表示并行度2，随机访问，重复测试3次，最大访问到128MB数据，步长为256（注意stride可以写多个，会依次进行测试，例如`./lat_mem_rd 128 64 256 1024`，会依次测试步长为64B、256B、1024B）

#### 执行如下指令进行测试

```shell
# loca dram
numactl -c 1 -m 1 ./lat_mem_rd -t -P 1 -N 10 1024M 64 
# remote NUMA
numactl -c 1 -m 0 ./lat_mem_rd -t -P 1 -N 10 1024M 64
# CXL memory
numactl -c 1 -m 2 ./lat_mem_rd -t -P 1 -N 10 1024M 64
```

#### 编译过程中可能出现的问题解决

![image-20240702123106088](/Users/hong/Library/Application%20Support/typora-user-images/image-20240702123106088.png)

这里中间的build中失败是因为中间的CFLAGS中的-I和后面的/usr/include/tirpc路径中间没有空格导致，改正之后可以通过编译。

成功，make results即可





#### 参考：

性能测试工具lmbench的使用方法以及解析运行结果

https://blog.csdn.net/qq_36393978/article/details/125989992

lmbench fatal error: rpc/rpc.h: No such file or directory

https://blog.csdn.net/qq_38963393/article/details/131715454

fix compilation error 'fatal error: rpc/rpc.h: No such file or directory' #16

https://github.com/intel/lmbench/issues/16

lmbench内存延迟测试代码分析https://developer.aliyun.com/article/591720

## 步骤三： STREAM测试带宽

```shell
# 编译指令
gcc -mtune=native -march=native -O3 -mcmodel=medium -fopenmp -DSTREAM_ARRAY_SIZE=512000000 -DNTIMES=10  stream.c -o stream512M.o

# -O3: 指定最高编译优化，3
# -DSTREAM_ARRAY_SIZE 必须设置数组大小远大于LLC的大小，否则测试的是CPU缓存的吞吐性能，而非内存吞吐性能
#  数组大小应该足够大，以便程序输出的“timing calibration”至少为20个时钟滴答。举例来说：
#  大多数Windows版本具有10毫秒的计时器粒度。在10毫秒/滴答下，20个“ticks”为200毫秒。如果芯片能够以10 #  GB/s的速度传输数据，在200毫秒内可传输2 GB。这意味着每个数组至少应该是1 GB，或者128M个元素。
# -fopenmp: 启用OpenMP，适应多处理器环境，更能得到内存带宽实际最大值。开启后，程序默认运行线程为CPU线程数；
# -mtune=native -march=native 针对CPU指令的优化，此处由于编译机即运行机器，故使用native的优化方法，更多编译器对CPU的优化参考
# -mcmodel=medium 当单个Memory Array Size大于2GB时需要设置此参数
# -DOFFSET 数组的偏移，一般可以不定义

# core number = 1
OMP_PLACES=cores OMP_PROC_BIND=spread numactl --physcpubind=12 --membind=2 ./stream512M.o | grep -E "Co
py|Scale|Add|Triad"
# 注意这里的physcpubind参数代表物理核，如果测试CXL memory的最大带宽，先跑不同数量的physcpubind，看一下几个cores的时候带宽达到最大
# core number = 2
OMP_PLACES=cores OMP_PROC_BIND=spread numactl --physcpubind=12-13 --membind=2 ./stream512M.o | grep -E "Co
py|Scale|Add|Triad"
# core number = 3 
OMP_PLACES=cores OMP_PROC_BIND=spread numactl --physcpubind=12-14 --membind=2 ./stream512M.o | grep -E "Co
py|Scale|Add|Triad"

# 按照以上方法，找到CXL memory的最大带宽
```

以上为CXL memory的测试，可以通过修改numactl --physcpubind和--membind控制绑定的CPU core和memory测试不同的内存的带宽，应该需要测试4组：

1. Local DRAM
2. Remote NUMA DRAM
3. Local FPGA CXL Memory
4. Local ASIC CXL Memory



## 步骤四：Redis-YCSB测试真实应用的QPS

直接将旧服务器上代码拷贝过去即可，注意运行Redis需要预装的package：

```shell
#  安装java和maven
sudo apt-get install maven
sudo apt-get install java
```

Redis配置文件路径为redis.conf

其中的Port部分如下，已经修改成6666，暂时不需要修改；292行生成的pid文件没有修改成和Port number一样的值，暂时不需要考虑。

**先将所有ycsb目录下的result*.txt导出到本地进行保存而后删除服务器上的result*.txt数据.**

**直接运行脚本即可，注意要在root权限下运行！**

```shell
sudo ./test_new.sh
```

运行脚本中重要部分截取，**注意将其中相关文件的目录修改为当前新服务器上的安装目录**，主要思路是在对应的CPU node和memory node上启动Redis，然后通过YCSB中不同类型的workloads对Redis进行操作：

```shell
echo "————————————————————workloadE 100% 本地DDR DRAM:————————————————————">> result_limit_swap.txt 
numactl --cpunodebind=1 --membind=1   /home/hwt/workspace/redis-6.2.14/src/redis-server /home/hwt/workspace/redis-6.2.14/redis.conf 

./bin/ycsb.sh load  redis -s -P workloads/workloade -p "redis.host=127.0.0.1" -p "redis.port=6666">> result_limit_swap.txt 
./bin/ycsb.sh run  redis -s -P workloads/workloade -p "redis.host=127.0.0.1" -p "redis.port=6666">> result_limit_swap.txt 

/home/hwt/workspace/redis-6.2.14/src/redis-cli -p 6666 shutdown
```

不同workload的配置文件在workloads目录下，其中主要是一些record数目和不同比例的read, update, scan, insert操作的配置。**record数目可能需要修改，但是不同读写比例的配置不需要修改**。







